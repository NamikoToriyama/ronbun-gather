import time
from deepl_translator import DeepLTranslator
from arxiv_scraper import ArxivScraper
from line_notifier import LineNotifier
from notion_saver import NotionSaver

# ========================================
# è¨­å®š
# ========================================

# æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆé–¢å¿ƒã®ã‚ã‚‹ç ”ç©¶åˆ†é‡ã‚’è¨­å®šï¼‰
SEARCH_KEYWORDS = [
    "typhoon intensity prediction",
    "tropical cyclone track forecast", 
    "hurricane eye wall replacement",
    "typhoon landfall prediction",
]

# å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§å–å¾—ã™ã‚‹è«–æ–‡æ•°
PAPERS_PER_KEYWORD = 1

# 1æ—¥ã«å‡¦ç†ã™ã‚‹æ–°è¦è«–æ–‡ã®ä¸Šé™æ•°ï¼ˆã“ã®æ•°ã«é”ã—ãŸã‚‰çµ‚äº†ï¼‰
MAX_NEW_PAPERS_PER_DAY = 2

# ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é–“ã®å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰
MESSAGE_INTERVAL = 2
QUERY_INTERVAL = 5

class PaperNotificationSystem:
    def __init__(self):
        # å„APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
        self.deepl = DeepLTranslator()
        self.arxiv = ArxivScraper()
        self.line = LineNotifier()
        self.notion = NotionSaver()
        
        # æ—¢å­˜ã®URLã‚’å–å¾—ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ï¼‰
        self.existing_urls = self.notion.get_existing_urls() if self.notion.is_enabled() else set()
        
        # å‡¦ç†æ¸ˆã¿æ–°è¦è«–æ–‡æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        self.processed_new_papers = 0
        
        print("ğŸš€ Paper Notification System initialized")
    
    def is_relevant_paper(self, paper, query):
        """è«–æ–‡ãŒã‚¯ã‚¨ãƒªã«é–¢é€£ã—ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        # é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆå°é¢¨ãƒ»ç†±å¸¯ä½æ°—åœ§é–¢é€£ï¼‰
        typhoon_keywords = [
            'typhoon', 'tropical cyclone', 'hurricane', 'cyclone',
            'storm', 'weather forecasting', 'meteorology', 'atmospheric',
            'precipitation', 'wind', 'satellite', 'climate', 'prediction',
            'å°é¢¨', 'ç†±å¸¯ä½æ°—åœ§', 'æ°—è±¡', 'äºˆå ±', 'äºˆæ¸¬'
        ]
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã¨è¦ç´„ã‚’çµåˆã—ã¦ãƒã‚§ãƒƒã‚¯
        text_to_check = (paper.get('title', '') + ' ' + paper.get('abstract', '')).lower()
        
        # ã‚¯ã‚¨ãƒªã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        query_words = query.lower().split()
        query_match = any(word in text_to_check for word in query_words)
        
        # å°é¢¨é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        keyword_match = any(keyword.lower() in text_to_check for keyword in typhoon_keywords)
        
        return query_match or keyword_match
    
    def filter_relevant_papers(self, papers, query):
        """é–¢é€£æ€§ã®é«˜ã„è«–æ–‡ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        relevant_papers = []
        
        for paper in papers:
            # é–¢é€£æ€§ãƒã‚§ãƒƒã‚¯
            if not self.is_relevant_paper(paper, query):
                print(f"âŒ Not relevant: {paper['title'][:60]}...")
                print(f"   Categories: {paper.get('categories', [])}")
                continue
            
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
            paper_url = paper.get('pdf_url') or paper.get('url')
            if self.notion.is_duplicate(paper_url, self.existing_urls):
                print(f"ğŸ”„ Duplicate (skipping): {paper['title'][:60]}...")
                print(f"   URL: {paper_url}")
                continue
            
            # é–¢é€£æ€§ãŒã‚ã‚Šã€é‡è¤‡ã§ãªã„è«–æ–‡ã®ã¿è¿½åŠ 
            relevant_papers.append(paper)
            print(f"âœ… New relevant paper: {paper['title'][:60]}...")
        
        return relevant_papers
    
    def process_single_paper(self, paper, paper_num, total_papers):
        """å˜ä¸€ã®è«–æ–‡ã‚’å‡¦ç†ï¼ˆç¿»è¨³ãƒ»LINEãƒ»Notionï¼‰"""
        print(f"\nğŸ“„ Processing paper {paper_num}/{total_papers}: {paper['title'][:50]}...")
        
        # DeepLä½¿ç”¨çŠ¶æ³ç¢ºèª
        usage = self.deepl.get_usage()
        remaining = usage['remaining']
        
        # Abstractç¿»è¨³
        abstract_chars = len(paper.get('abstract', ''))
        if remaining >= abstract_chars:
            print(f"ğŸ”„ Translating abstract ({abstract_chars} chars)...")
            translated = self.deepl.translate_abstract(paper['abstract'])
            if translated:
                paper['translated_abstract'] = translated
                print(f"âœ… Translation completed")
            else:
                print(f"âŒ Translation failed")
        else:
            print(f"âš ï¸ Skipping translation (insufficient quota: need {abstract_chars}, have {remaining})")
            paper['translated_abstract'] = None
        
        # LINEã«é€ä¿¡
        # 1ã¤ç›®ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆåŸºæœ¬æƒ…å ±ï¼‰
        basic_message = self.line.format_basic_info_message(paper, paper_num, total_papers)
        self.line.send_message(basic_message)
        time.sleep(MESSAGE_INTERVAL)
        
        # 2ã¤ç›®ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆç¿»è¨³ã—ãŸè¦ç´„ï¼‰
        abstract_message = self.line.format_abstract_message(paper)
        self.line.send_message(abstract_message)
        time.sleep(MESSAGE_INTERVAL)
        
        # Notionã«ä¿å­˜
        if self.notion.is_enabled():
            notion_page = self.notion.save_paper(paper)
            if notion_page:
                print(f"ğŸ“ Notion page created: {notion_page['id']}")
                # æˆåŠŸã—ãŸå ´åˆã€æ—¢å­˜URLãƒªã‚¹ãƒˆã«è¿½åŠ ï¼ˆåŒã˜ã‚»ãƒƒã‚·ãƒ§ãƒ³å†…ã§ã®é‡è¤‡é˜²æ­¢ï¼‰
                paper_url = paper.get('pdf_url') or paper.get('url')
                if paper_url:
                    self.existing_urls.add(paper_url)
            else:
                print(f"âŒ Failed to save to Notion")
        
        print(f"âœ… Paper {paper_num} processing completed")
        
        # æ–°è¦è«–æ–‡æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        self.processed_new_papers += 1
    
    def search_translate_and_notify(self, query, max_results=2):
        """è«–æ–‡ã‚’æ¤œç´¢ã€ç¿»è¨³ã—ã¦LINEãƒ»Notionã§é€šçŸ¥"""
        print(f"ğŸ” Starting paper search for: '{query}'")
        
        # arXivã§æ¤œç´¢ï¼ˆå¤šã‚ã«å–å¾—ã—ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰
        papers = self.arxiv.search_papers(query, max_results * 3)
        
        if not papers:
            print("âŒ No papers found")
            self.line.send_message(f"ã€Œ{query}ã€ã«é–¢ã™ã‚‹è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return
        
        print(f"âœ… Found {len(papers)} papers")
        
        # é–¢é€£æ€§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        relevant_papers = self.filter_relevant_papers(papers, query)
        
        if not relevant_papers:
            print("âŒ No relevant papers found after filtering")
            self.line.send_message(f"ã€Œ{query}ã€ã«é–¢é€£ã™ã‚‹è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return
        
        # å¿…è¦ãªæ•°ã¾ã§çµã‚Šè¾¼ã¿
        papers = relevant_papers[:max_results]
        print(f"ğŸ“‹ Using {len(papers)} relevant papers")
        
        # DeepLä½¿ç”¨çŠ¶æ³ç¢ºèª
        usage = self.deepl.get_usage()
        print(f"ğŸ“Š DeepL usage: {usage['used']:,} / {usage['limit']:,} characters")
        print(f"ğŸ“Š Remaining: {usage['remaining']:,} characters")
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡
        header = f"ğŸ”¬ æœ¬æ—¥ã®è«–æ–‡æƒ…å ± ({len(papers)}ä»¶)\n" + "="*30
        self.line.send_message(header)
        time.sleep(1)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
        
        # å„è«–æ–‡ã‚’å‡¦ç†
        for i, paper in enumerate(papers, 1):
            self.process_single_paper(paper, i, len(papers))
        
        print(f"\nğŸ‰ All {len(papers)} papers processed and sent!")
        
        # æ–°è¦è«–æ–‡æ•°ã®ä¸Šé™ãƒã‚§ãƒƒã‚¯
        return self.processed_new_papers >= MAX_NEW_PAPERS_PER_DAY

def main():
    try:
        print("ğŸš€ Starting Paper Notification System...")
        print(f"ğŸ“‹ Search keywords: {', '.join(SEARCH_KEYWORDS)}")
        print(f"ğŸ“Š Papers per keyword: {PAPERS_PER_KEYWORD}")
        
        # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        system = PaperNotificationSystem()
        
        for query in SEARCH_KEYWORDS:
            print(f"\n{'='*60}")
            print(f"Processing query: {query}")
            print(f"æ–°è¦è«–æ–‡å‡¦ç†æ•°: {system.processed_new_papers}/{MAX_NEW_PAPERS_PER_DAY}")
            print('='*60)
            
            # ä¸Šé™ã«é”ã—ã¦ã„ãŸã‚‰çµ‚äº†
            if system.processed_new_papers >= MAX_NEW_PAPERS_PER_DAY:
                print(f"ğŸ¯ Daily limit reached ({MAX_NEW_PAPERS_PER_DAY} new papers processed)")
                break
            
            should_stop = system.search_translate_and_notify(query, max_results=PAPERS_PER_KEYWORD)
            
            print(f"âœ… Query '{query}' completed")
            
            # ä¸Šé™ã«é”ã—ãŸã‚‰çµ‚äº†
            if should_stop:
                print(f"ğŸ¯ Daily limit reached ({MAX_NEW_PAPERS_PER_DAY} new papers processed)")
                break
                
            time.sleep(QUERY_INTERVAL)
        
        print(f"\nğŸ‰ Paper Notification System completed successfully!")
        
    except Exception as e:
        print(f"âŒ System error: {e}")
        if "DEEPL_API_KEY" in str(e):
            print("Please set your DeepL API key in .env file")
        elif "LINE_CHANNEL_ACCESS_TOKEN" in str(e):
            print("Please set your LINE Bot credentials in .env file")

if __name__ == "__main__":
    main()
